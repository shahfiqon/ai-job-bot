# Job Apply Assistant Bot - Backend API

This FastAPI service powers the Job Apply Assistant Bot, exposing endpoints that coordinate job scraping, enrichment via Proxycurl, and future workflow automation.

## Prerequisites
- Python 3.10+
- [PDM](https://pdm.fming.dev/latest/)
- Docker & Docker Compose

## Setup
1. Install PDM if needed: `pip install pdm`
2. Change into the backend directory: `cd backend`
3. Install dependencies: `pdm install`
4. Copy `.env.example` from repository root to `backend/.env` and update the settings
5. Start PostgreSQL locally (from the repository root): `docker-compose up -d`
6. Run database migrations: `alembic upgrade head`
7. Launch the API: `pdm run uvicorn app.main:app --reload --host 0.0.0.0 --port 8000`

Open http://localhost:8000/docs for the autogenerated Swagger UI. Health checks are available at http://localhost:8000/health.

## Database Setup
PostgreSQL runs inside Docker using the root-level `docker-compose.yml`.

- Start the database (run from the repository root): `docker-compose up -d`
- Verify the container is healthy: `docker-compose ps`
- Connect to the database shell: `docker exec -it jobbot-postgres psql -U postgres -d jobbot`

Ensure the `.env` file inside `backend/` contains the correct `DATABASE_URL` before running migrations or launching the API.

## Database Migrations
Alembic manages schema changes for the backend service. All commands below are executed from the `backend/` directory unless noted otherwise.

- Apply the latest migrations: `alembic upgrade head`
- Generate a new migration from model changes: `alembic revision --autogenerate -m "description"`
- Roll back the previous migration: `alembic downgrade -1`
- Show migration history: `alembic history`
- Show the current database head: `alembic current`

## CLI Tools
The backend includes a Typer-powered CLI for scraping jobs, enriching company profiles via Proxycurl, and storing everything in PostgreSQL with rich progress feedback.

### Installation
- Ensure you're inside the backend directory: `cd backend`
- Install dependencies: `pdm install`
- Run the CLI with PDM: `pdm run jobbot`

### Available Commands
1. **Scrape Jobs**

   ```bash
   pdm run jobbot scrape --search-term "software engineer" --location "San Francisco, CA" --results-wanted 20
   ```

   **Parameters**
   - `--search-term` (required): e.g., "software engineer", "data scientist"
   - `--location` (required): e.g., "San Francisco, CA", "New York, NY", "Remote"
   - `--results-wanted` (optional, default 15): number of listings to request (max recommended 50)

   **Behavior**
   - Scrapes LinkedIn via jobspy
   - Extracts LinkedIn company URLs and enriches them with Proxycurl (DB cached)
   - Persists both companies and jobs in PostgreSQL
   - Renders progress bars plus a summary table with Rich

   **Example Output**
   ```
   ╭─────────────────────────────────────────╮
   │ Job Scraping Configuration              │
   │ Search: software engineer               │
   │ Location: San Francisco, CA             │
   │ Results: 20                             │
   ╰─────────────────────────────────────────╯

   ✓ Scraping jobs from LinkedIn... Done!
   ✓ Enriching company profiles... Done!
   ✓ Storing jobs in database... Done!

   ┌────────────────────────────┬───────┐
   │ Metric                     │ Count │
   ├────────────────────────────┼───────┤
   │ Jobs Scraped               │ 20    │
   │ Jobs Created               │ 18    │
   │ Jobs Skipped (duplicates)  │ 2     │
   │ Companies Found            │ 15    │
   │ Companies Created          │ 10    │
   │ Companies Cached           │ 5     │
   │ Companies Failed           │ 0     │
   └────────────────────────────┴───────┘
   ```

### Prerequisites
- PostgreSQL running via `docker-compose up -d` (from repo root)
- Migrations applied: `alembic upgrade head`
- `.env` file with:
  - `DATABASE_URL`
  - `PROXYCURL_API_KEY` (from https://nubela.co/proxycurl)

### Troubleshooting
- **"No jobs found"**: adjust search term/location or raise `--results-wanted`
- **"Proxycurl API error 401"**: confirm `PROXYCURL_API_KEY`
- **"Proxycurl API error 429"**: you're rate limited; wait a few minutes
- **"Database connection error"**: ensure PostgreSQL is running and `DATABASE_URL` is correct
- **"No LinkedIn URLs found"**: try different search parameters

### Notes
- Companies are deduplicated by LinkedIn URL before calling Proxycurl
- Jobs are deduplicated by `job_url`
- Company enrichment data is cached in PostgreSQL, so repeat scrapes skip API calls
- Rich-powered output requires a terminal that supports ANSI colors

## API Endpoints

The FastAPI service exposes REST endpoints consumed by the Next.js frontend.

- **Base URL:** `http://localhost:8000`
- **Interactive Docs:** `http://localhost:8000/docs`

### `GET /api/jobs`
- Lists jobs ordered by most recent posting.
- **Query parameters**
  - `page` (int, default `1`, min `1`)
  - `page_size` (int, default `20`, min `1`, max `100`)
- **Response:** `JobListResponse`
  - `jobs`: array of job objects
  - `total`: total number of jobs
  - `page`: current page number
  - `page_size`: items per page
  - `total_pages`: total page count
- **Example:** `GET /api/jobs?page=1&page_size=20`

### `GET /api/jobs/{job_id}`
- Returns a single job with optional company details.
- **Path parameter**
  - `job_id` (int): primary key of the job.
- **Response:** `JobDetailResponse`
  - All job fields
  - `company`: `CompanyResponse | null`
- **Errors**
  - `404` if the job is missing.
  - `500` for unexpected server/database issues.
- **Example:** `GET /api/jobs/1`

### CORS
- Configure allowed origins via the `CORS_ORIGINS` environment variable (comma-separated). Defaults cover both `http://localhost:3000` and `http://127.0.0.1:3000` for local development.
- Allowed methods/headers: `*`
- Credentials: enabled

### Data Format
- JSON responses using ISO 8601 timestamps (e.g., `2025-01-15T10:30:00Z`)
- JSONB fields (`job_type`, `emails`, `specialities`) are serialized as arrays

### Testing the API
1. Start the backend: `pdm run uvicorn app.main:app --reload`
2. Visit `http://localhost:8000/docs` to exercise the endpoints.
3. Or use `curl`:
   ```bash
   curl "http://localhost:8000/api/jobs?page=1&page_size=10"
   curl "http://localhost:8000/api/jobs/1"
   ```

> **Note:** The API expects PostgreSQL to be running with seeded job data. If the database is empty the list endpoint will simply return an empty array.

## Database Models
New SQLAlchemy ORM models live in `app/models/`:

- `Company`: stores Proxycurl-enriched company metadata keyed by LinkedIn URL (unique). Includes headquarters, size ranges, industry, website, profile imagery, and raw JSON blobs for specialities and offices.
- `Job`: stores scraped job postings returned by jobspy, normalized into columns for location, compensation, posting metadata, and contact info. `Job.company_id` is nullable to support enrichment after scraping, with a many-to-one relationship to `Company`.

## Project Structure
- `app/main.py` – FastAPI instance, middleware, and placeholder routes
- `app/config.py` – Pydantic settings that load database and Proxycurl secrets
- `app/models/` – SQLAlchemy ORM models for jobs and companies
- `pyproject.toml` – PDM project definition and dependency list

## Troubleshooting
- If migrations fail, confirm PostgreSQL is running via `docker-compose ps` and that `DATABASE_URL` in `.env` points to the running instance.
- If `alembic revision --autogenerate` produces an empty migration, verify the models are imported in `app/models/__init__.py` so Alembic can discover their metadata.
- To completely reset the database (destructive): `docker-compose down -v`, then `docker-compose up -d`, followed by `alembic upgrade head`.

CLI utilities and additional endpoints will be added in upcoming phases.
